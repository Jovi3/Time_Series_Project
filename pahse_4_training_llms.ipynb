{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jovitagandhi/opt/anaconda3/envs/tfarm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4557/4557 [00:00<00:00, 140853.84 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 507/507 [00:00<00:00, 86184.33 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4557/4557 [00:00<00:00, 166990.89 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 507/507 [00:00<00:00, 84078.45 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 4557\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 507\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 4557\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 507\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the digit-level dataset\n",
    "ds_digit = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"dataset_gruver_digit.jsonl\"\n",
    ")\n",
    "\n",
    "# Load the numeric-token dataset\n",
    "ds_numeric = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"dataset_delphyne_numeric.jsonl\"\n",
    ")\n",
    "\n",
    "# Train/validation split (90/10)\n",
    "digit_ds = ds_digit[\"train\"].train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "numeric_ds = ds_numeric[\"train\"].train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "\n",
    "# Save dataset dictionaries for future use\n",
    "digit_ds.save_to_disk(\"hf_digit_dataset\")\n",
    "numeric_ds.save_to_disk(\"hf_numeric_dataset\")\n",
    "\n",
    "print(digit_ds)\n",
    "print(numeric_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers datasets accelerate bitsandbytes peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "numeric_ds = load_from_disk(\"hf_numeric_dataset\")\n",
    "digit_ds   = load_from_disk(\"hf_digit_dataset\")   # will use later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sample(example):\n",
    "    example[\"text\"] = f\"Predict the next return.\\nSequence: {example['input_text']}\\nNext: {example['target_text']}\"\n",
    "    return example\n",
    "\n",
    "train_ds = numeric_ds[\"train\"].map(format_sample)\n",
    "val_ds   = numeric_ds[\"test\"].map(format_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Predict the next return.\\nSequence: 0.011956 0.011265 0.01152 0.011877 0.005572 0.006475 0.004829 0.00628 0.004783 0.006473 0.005965 0.023039 0.010506 0.016359 0.010665 0.017357 0.01001 0.01093 0.006556 0.018859 0.017817 0.019164 0.018153 0.01987 0.019268 0.084112 0.009394 0.013847 0.009123 0.011749 0.011267 0.008967 0.056426 0.001753 0.003005 0.0023 0.004079 0.000418 0.002405 -0.016568 0.00284 0.008956 0.002846 0.009453 0.00297 0.00265 0.028665 0.001005 0.000773 0.000732 0.000644 -0.000139 0.00082 -0.013933 -0.008084 -0.009447 -0.007799 -0.007696 -0.008514 -0.007537\\nNext: -0.030987'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade --force-reinstall git+https://github.com/huggingface/transformers.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 4557\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 507\n",
       " }))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Make sure padding is correct\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    tok = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,   # you can increase later\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels = input_ids\n",
    "    tok[\"labels\"] = tok[\"input_ids\"].copy()\n",
    "    return tok\n",
    "\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True, remove_columns=train_ds.column_names)\n",
    "val_tok   = val_ds.map(tokenize_fn, batched=True, remove_columns=val_ds.column_names)\n",
    "\n",
    "train_tok, val_tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn\n",
      "model.layers.0.self_attn.o_proj\n",
      "model.layers.0.self_attn.qkv_proj\n",
      "model.layers.0.mlp.gate_up_proj\n",
      "model.layers.0.mlp.down_proj\n",
      "model.layers.0.resid_attn_dropout\n",
      "model.layers.1.self_attn\n",
      "model.layers.1.self_attn.o_proj\n",
      "model.layers.1.self_attn.qkv_proj\n",
      "model.layers.1.mlp.gate_up_proj\n",
      "model.layers.1.mlp.down_proj\n",
      "model.layers.1.resid_attn_dropout\n",
      "model.layers.2.self_attn\n",
      "model.layers.2.self_attn.o_proj\n",
      "model.layers.2.self_attn.qkv_proj\n",
      "model.layers.2.mlp.gate_up_proj\n",
      "model.layers.2.mlp.down_proj\n",
      "model.layers.2.resid_attn_dropout\n",
      "model.layers.3.self_attn\n",
      "model.layers.3.self_attn.o_proj\n",
      "model.layers.3.self_attn.qkv_proj\n",
      "model.layers.3.mlp.gate_up_proj\n",
      "model.layers.3.mlp.down_proj\n",
      "model.layers.3.resid_attn_dropout\n",
      "model.layers.4.self_attn\n",
      "model.layers.4.self_attn.o_proj\n",
      "model.layers.4.self_attn.qkv_proj\n",
      "model.layers.4.mlp.gate_up_proj\n",
      "model.layers.4.mlp.down_proj\n",
      "model.layers.4.resid_attn_dropout\n",
      "model.layers.5.self_attn\n",
      "model.layers.5.self_attn.o_proj\n",
      "model.layers.5.self_attn.qkv_proj\n",
      "model.layers.5.mlp.gate_up_proj\n",
      "model.layers.5.mlp.down_proj\n",
      "model.layers.5.resid_attn_dropout\n",
      "model.layers.6.self_attn\n",
      "model.layers.6.self_attn.o_proj\n",
      "model.layers.6.self_attn.qkv_proj\n",
      "model.layers.6.mlp.gate_up_proj\n",
      "model.layers.6.mlp.down_proj\n",
      "model.layers.6.resid_attn_dropout\n",
      "model.layers.7.self_attn\n",
      "model.layers.7.self_attn.o_proj\n",
      "model.layers.7.self_attn.qkv_proj\n",
      "model.layers.7.mlp.gate_up_proj\n",
      "model.layers.7.mlp.down_proj\n",
      "model.layers.7.resid_attn_dropout\n",
      "model.layers.8.self_attn\n",
      "model.layers.8.self_attn.o_proj\n",
      "model.layers.8.self_attn.qkv_proj\n",
      "model.layers.8.mlp.gate_up_proj\n",
      "model.layers.8.mlp.down_proj\n",
      "model.layers.8.resid_attn_dropout\n",
      "model.layers.9.self_attn\n",
      "model.layers.9.self_attn.o_proj\n",
      "model.layers.9.self_attn.qkv_proj\n",
      "model.layers.9.mlp.gate_up_proj\n",
      "model.layers.9.mlp.down_proj\n",
      "model.layers.9.resid_attn_dropout\n",
      "model.layers.10.self_attn\n",
      "model.layers.10.self_attn.o_proj\n",
      "model.layers.10.self_attn.qkv_proj\n",
      "model.layers.10.mlp.gate_up_proj\n",
      "model.layers.10.mlp.down_proj\n",
      "model.layers.10.resid_attn_dropout\n",
      "model.layers.11.self_attn\n",
      "model.layers.11.self_attn.o_proj\n",
      "model.layers.11.self_attn.qkv_proj\n",
      "model.layers.11.mlp.gate_up_proj\n",
      "model.layers.11.mlp.down_proj\n",
      "model.layers.11.resid_attn_dropout\n",
      "model.layers.12.self_attn\n",
      "model.layers.12.self_attn.o_proj\n",
      "model.layers.12.self_attn.qkv_proj\n",
      "model.layers.12.mlp.gate_up_proj\n",
      "model.layers.12.mlp.down_proj\n",
      "model.layers.12.resid_attn_dropout\n",
      "model.layers.13.self_attn\n",
      "model.layers.13.self_attn.o_proj\n",
      "model.layers.13.self_attn.qkv_proj\n",
      "model.layers.13.mlp.gate_up_proj\n",
      "model.layers.13.mlp.down_proj\n",
      "model.layers.13.resid_attn_dropout\n",
      "model.layers.14.self_attn\n",
      "model.layers.14.self_attn.o_proj\n",
      "model.layers.14.self_attn.qkv_proj\n",
      "model.layers.14.mlp.gate_up_proj\n",
      "model.layers.14.mlp.down_proj\n",
      "model.layers.14.resid_attn_dropout\n",
      "model.layers.15.self_attn\n",
      "model.layers.15.self_attn.o_proj\n",
      "model.layers.15.self_attn.qkv_proj\n",
      "model.layers.15.mlp.gate_up_proj\n",
      "model.layers.15.mlp.down_proj\n",
      "model.layers.15.resid_attn_dropout\n",
      "model.layers.16.self_attn\n",
      "model.layers.16.self_attn.o_proj\n",
      "model.layers.16.self_attn.qkv_proj\n",
      "model.layers.16.mlp.gate_up_proj\n",
      "model.layers.16.mlp.down_proj\n",
      "model.layers.16.resid_attn_dropout\n",
      "model.layers.17.self_attn\n",
      "model.layers.17.self_attn.o_proj\n",
      "model.layers.17.self_attn.qkv_proj\n",
      "model.layers.17.mlp.gate_up_proj\n",
      "model.layers.17.mlp.down_proj\n",
      "model.layers.17.resid_attn_dropout\n",
      "model.layers.18.self_attn\n",
      "model.layers.18.self_attn.o_proj\n",
      "model.layers.18.self_attn.qkv_proj\n",
      "model.layers.18.mlp.gate_up_proj\n",
      "model.layers.18.mlp.down_proj\n",
      "model.layers.18.resid_attn_dropout\n",
      "model.layers.19.self_attn\n",
      "model.layers.19.self_attn.o_proj\n",
      "model.layers.19.self_attn.qkv_proj\n",
      "model.layers.19.mlp.gate_up_proj\n",
      "model.layers.19.mlp.down_proj\n",
      "model.layers.19.resid_attn_dropout\n",
      "model.layers.20.self_attn\n",
      "model.layers.20.self_attn.o_proj\n",
      "model.layers.20.self_attn.qkv_proj\n",
      "model.layers.20.mlp.gate_up_proj\n",
      "model.layers.20.mlp.down_proj\n",
      "model.layers.20.resid_attn_dropout\n",
      "model.layers.21.self_attn\n",
      "model.layers.21.self_attn.o_proj\n",
      "model.layers.21.self_attn.qkv_proj\n",
      "model.layers.21.mlp.gate_up_proj\n",
      "model.layers.21.mlp.down_proj\n",
      "model.layers.21.resid_attn_dropout\n",
      "model.layers.22.self_attn\n",
      "model.layers.22.self_attn.o_proj\n",
      "model.layers.22.self_attn.qkv_proj\n",
      "model.layers.22.mlp.gate_up_proj\n",
      "model.layers.22.mlp.down_proj\n",
      "model.layers.22.resid_attn_dropout\n",
      "model.layers.23.self_attn\n",
      "model.layers.23.self_attn.o_proj\n",
      "model.layers.23.self_attn.qkv_proj\n",
      "model.layers.23.mlp.gate_up_proj\n",
      "model.layers.23.mlp.down_proj\n",
      "model.layers.23.resid_attn_dropout\n",
      "model.layers.24.self_attn\n",
      "model.layers.24.self_attn.o_proj\n",
      "model.layers.24.self_attn.qkv_proj\n",
      "model.layers.24.mlp.gate_up_proj\n",
      "model.layers.24.mlp.down_proj\n",
      "model.layers.24.resid_attn_dropout\n",
      "model.layers.25.self_attn\n",
      "model.layers.25.self_attn.o_proj\n",
      "model.layers.25.self_attn.qkv_proj\n",
      "model.layers.25.mlp.gate_up_proj\n",
      "model.layers.25.mlp.down_proj\n",
      "model.layers.25.resid_attn_dropout\n",
      "model.layers.26.self_attn\n",
      "model.layers.26.self_attn.o_proj\n",
      "model.layers.26.self_attn.qkv_proj\n",
      "model.layers.26.mlp.gate_up_proj\n",
      "model.layers.26.mlp.down_proj\n",
      "model.layers.26.resid_attn_dropout\n",
      "model.layers.27.self_attn\n",
      "model.layers.27.self_attn.o_proj\n",
      "model.layers.27.self_attn.qkv_proj\n",
      "model.layers.27.mlp.gate_up_proj\n",
      "model.layers.27.mlp.down_proj\n",
      "model.layers.27.resid_attn_dropout\n",
      "model.layers.28.self_attn\n",
      "model.layers.28.self_attn.o_proj\n",
      "model.layers.28.self_attn.qkv_proj\n",
      "model.layers.28.mlp.gate_up_proj\n",
      "model.layers.28.mlp.down_proj\n",
      "model.layers.28.resid_attn_dropout\n",
      "model.layers.29.self_attn\n",
      "model.layers.29.self_attn.o_proj\n",
      "model.layers.29.self_attn.qkv_proj\n",
      "model.layers.29.mlp.gate_up_proj\n",
      "model.layers.29.mlp.down_proj\n",
      "model.layers.29.resid_attn_dropout\n",
      "model.layers.30.self_attn\n",
      "model.layers.30.self_attn.o_proj\n",
      "model.layers.30.self_attn.qkv_proj\n",
      "model.layers.30.mlp.gate_up_proj\n",
      "model.layers.30.mlp.down_proj\n",
      "model.layers.30.resid_attn_dropout\n",
      "model.layers.31.self_attn\n",
      "model.layers.31.self_attn.o_proj\n",
      "model.layers.31.self_attn.qkv_proj\n",
      "model.layers.31.mlp.gate_up_proj\n",
      "model.layers.31.mlp.down_proj\n",
      "model.layers.31.resid_attn_dropout\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if \"proj\" in name.lower() or \"attn\" in name.lower():\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'partial_rotary_factor'}\n",
      "Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'partial_rotary_factor'}\n",
      "Loading weights: 100%|██████████| 195/195 [00:25<00:00,  7.78it/s, Materializing param=model.norm.weight]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on CPU.\n",
      "trainable params: 4,718,592 || all params: 3,825,798,144 || trainable%: 0.1233\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model in float32 on CPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map={\"\": \"cpu\"},   # force CPU\n",
    "    torch_dtype=\"float32\"\n",
    ")\n",
    "\n",
    "print(\"Model loaded on CPU.\")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        \"self_attn.qkv_proj\",\n",
    "        \"self_attn.o_proj\"\n",
    "    ],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jovitagandhi/opt/anaconda3/envs/tfarm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy: 1.26.4\n",
      "Torch: 2.9.1\n",
      "Transformers: 5.0.0.dev0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, torch, transformers\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "# Dynamic padding collator for causal LM (no masking, next-token prediction)\n",
    "collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phi3_finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,   # effective batch = 16\n",
    "    num_train_epochs=3,\n",
    "    \n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=50,\n",
    "\n",
    "    logging_steps=10,\n",
    "    eval_steps=50,\n",
    "    \n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    bf16=False,\n",
    "    fp16=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    data_collator=collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c131da5094cdd70e064dab4e51fc7bd3e79b520ef5c49c74af192f6ff585e4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
